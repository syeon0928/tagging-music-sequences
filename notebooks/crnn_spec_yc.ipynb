{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T18:21:46.807323Z",
     "start_time": "2023-11-19T18:21:26.932274Z"
    },
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Set path variables\n",
    "import os\n",
    "import sys\n",
    "cwd = os.getcwd()\n",
    "project_dir = os.path.abspath(os.path.join(cwd, os.pardir))\n",
    "sys.path.append(project_dir)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "from src import audio_util\n",
    "from src.audio_dataset import AudioDS\n",
    "from src.trainer import Trainer\n",
    "from src.model_yc import CRNN1, CRNN2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd909e4f0f3729d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T18:21:52.919027Z",
     "start_time": "2023-11-19T18:21:52.915296Z"
    },
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is not available. Using CPU device.\n",
      "Selected device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Check if MPS is available\n",
    "# if torch.backends.mps.is_available():\n",
    "#     device = torch.device(\"mps\")\n",
    "#     print(\"Using MPS device.\")\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Using CPU device.\")\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(\"Selected device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1d4b919e451b545",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T18:21:57.090309Z",
     "start_time": "2023-11-19T18:21:57.081392Z"
    },
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Load label annotation csv\n",
    "train_annotations = 'mtat_train_label.csv'\n",
    "val_annotations = 'mtat_val_label.csv'\n",
    "test_annotations = 'mtat_test_label.csv'\n",
    "\n",
    "# Data path\n",
    "from pathlib import Path\n",
    "cwd = Path.cwd()\n",
    "DATA_DIR = cwd.parent / 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a51947bfa0d5ffe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T18:21:58.217190Z",
     "start_time": "2023-11-19T18:21:57.686960Z"
    },
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Transformations on dataset\n",
    "SAMPLE_RATE = 16000\n",
    "DURATION_IN_SEC = 29.1\n",
    "MEL_SPEC_DB_TRANSFORMATION = audio_util.get_audio_transforms(SAMPLE_RATE,\n",
    "                                                            n_fft=512,\n",
    "                                                            hop_length=256,\n",
    "                                                            n_mels=96,\n",
    "                                                            top_db=80)\n",
    "\n",
    "train_data = AudioDS(annotations_file=train_annotations,\n",
    "                     data_dir=DATA_DIR,\n",
    "                     target_sample_rate=SAMPLE_RATE,\n",
    "                     target_length=DURATION_IN_SEC,\n",
    "                     transformation=MEL_SPEC_DB_TRANSFORMATION)\n",
    "\n",
    "val_data = AudioDS(annotations_file=val_annotations,\n",
    "                   data_dir=DATA_DIR,\n",
    "                   target_sample_rate=SAMPLE_RATE,\n",
    "                   target_length=DURATION_IN_SEC,\n",
    "                   transformation=MEL_SPEC_DB_TRANSFORMATION)\n",
    "\n",
    "test_data = AudioDS(annotations_file=val_annotations,\n",
    "                    data_dir=DATA_DIR,\n",
    "                    target_sample_rate=SAMPLE_RATE,\n",
    "                    target_length=DURATION_IN_SEC,\n",
    "                    transformation=MEL_SPEC_DB_TRANSFORMATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "192e035be5eaab94",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T18:22:02.467502Z",
     "start_time": "2023-11-19T18:22:02.458299Z"
    },
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad2d909def5cd005",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T18:22:05.853223Z",
     "start_time": "2023-11-19T18:22:05.840031Z"
    },
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataloader = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_dataloader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ce2d4043561fffa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T18:22:09.195509Z",
     "start_time": "2023-11-19T18:22:07.634296Z"
    },
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([32, 1, 96, 1819])\n",
      "Labels batch shape: torch.Size([32, 50])\n"
     ]
    }
   ],
   "source": [
    "# Display batch information\n",
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d552d18239e8a2",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### CRNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Add description of architecture / reference"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b489b3b45994717",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T18:22:29.537172Z",
     "start_time": "2023-11-19T18:22:11.857916Z"
    },
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate model\n",
    "crnn = CRNN1()\n",
    "crnn.to(device)\n",
    "# Instantiate trainer\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(crnn.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "trainer = Trainer(crnn, train_dataloader, val_dataloader, criterion, optimizer, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 96, 1819])\n"
     ]
    }
   ],
   "source": [
    "input_size = (train_features.size()[1:])\n",
    "print(input_size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 96, 1819])\n",
      "conv1 output shape: torch.Size([2, 64, 96, 1819])\n",
      "conv2 output shape: torch.Size([2, 128, 96, 1819])\n",
      "conv3 output shape: torch.Size([2, 256, 96, 1819])\n",
      "conv4 output shape: torch.Size([2, 512, 96, 1819])\n",
      "torch.Size([2, 174624, 512])\n",
      "tensor([[[-0.0162, -0.0226, -0.0756,  ..., -0.1116, -0.0348, -0.0389],\n",
      "         [-0.1095,  0.0015, -0.0368,  ..., -0.1866, -0.0758, -0.0047],\n",
      "         [-0.0916, -0.0874,  0.0142,  ..., -0.2438,  0.0149,  0.1337],\n",
      "         ...,\n",
      "         [ 0.0324,  0.2763, -0.2798,  ...,  0.0146, -0.1977,  0.0671],\n",
      "         [ 0.0197,  0.2777, -0.2286,  ..., -0.0194, -0.1925,  0.0595],\n",
      "         [ 0.0266,  0.1213, -0.1164,  ...,  0.0738, -0.1751,  0.0830]],\n",
      "\n",
      "        [[-0.0453, -0.0666, -0.1230,  ..., -0.0122, -0.0248,  0.0468],\n",
      "         [ 0.0492,  0.0163, -0.1377,  ...,  0.0115, -0.1339,  0.1033],\n",
      "         [ 0.0690,  0.0697, -0.1436,  ..., -0.1535, -0.1475,  0.2917],\n",
      "         ...,\n",
      "         [-0.2315,  0.3587, -0.3228,  ...,  0.1154, -0.0143,  0.1384],\n",
      "         [-0.0845,  0.4635, -0.2484,  ...,  0.1888, -0.0217,  0.1247],\n",
      "         [-0.0487,  0.2657, -0.1360,  ...,  0.1936,  0.0794,  0.1657]]],\n",
      "       grad_fn=<TransposeBackward1>)\n",
      "gru2 output shape: torch.Size([2, 174624, 128])\n",
      "fc output shape: torch.Size([2, 50])\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 96, 1819]             640\n",
      "       BatchNorm2d-2         [-1, 64, 96, 1819]             128\n",
      "               ELU-3         [-1, 64, 96, 1819]               0\n",
      "         Dropout2d-4         [-1, 64, 96, 1819]               0\n",
      "            Conv2d-5        [-1, 128, 96, 1819]          73,856\n",
      "       BatchNorm2d-6        [-1, 128, 96, 1819]             256\n",
      "               ELU-7        [-1, 128, 96, 1819]               0\n",
      "         Dropout2d-8        [-1, 128, 96, 1819]               0\n",
      "            Conv2d-9        [-1, 256, 96, 1819]         295,168\n",
      "      BatchNorm2d-10        [-1, 256, 96, 1819]             512\n",
      "              ELU-11        [-1, 256, 96, 1819]               0\n",
      "        Dropout2d-12        [-1, 256, 96, 1819]               0\n",
      "           Conv2d-13        [-1, 512, 96, 1819]       1,180,160\n",
      "      BatchNorm2d-14        [-1, 512, 96, 1819]           1,024\n",
      "              ELU-15        [-1, 512, 96, 1819]               0\n",
      "        Dropout2d-16        [-1, 512, 96, 1819]               0\n",
      "              GRU-17  [[-1, 174624, 256], [-1, 2, 256]]               0\n",
      "              GRU-18  [[-1, 174624, 128], [-1, 2, 128]]               0\n",
      "           Linear-19                   [-1, 50]           6,450\n",
      "================================================================\n",
      "Total params: 1,558,194\n",
      "Trainable params: 1,558,194\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.67\n",
      "Forward/backward pass size (MB): 213164.06\n",
      "Params size (MB): 5.94\n",
      "Estimated Total Size (MB): 213170.67\n",
      "----------------------------------------------------------------\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#print(summary(crnn, input_size))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51faac7a19a03e31",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T19:33:21.025035Z",
     "start_time": "2023-11-19T18:24:06.480372Z"
    },
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 96, 1819])\n",
      "conv1 output shape: torch.Size([32, 64, 96, 1819])\n",
      "conv2 output shape: torch.Size([32, 128, 96, 1819])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/10 [02:04<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[11], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Run training\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mEPOCHS\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/Tagging-Music-Sequences/src/trainer.py:41\u001B[0m, in \u001B[0;36mTrainer.train\u001B[0;34m(self, epochs)\u001B[0m\n\u001B[1;32m     39\u001B[0m \u001B[38;5;66;03m# train\u001B[39;00m\n\u001B[1;32m     40\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m---> 41\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     42\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcriterion(outputs, labels)\n\u001B[1;32m     43\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n",
      "File \u001B[0;32m~/PycharmProjects/Tagging-Music-Sequences/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/Tagging-Music-Sequences/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/Tagging-Music-Sequences/src/model_yc.py:50\u001B[0m, in \u001B[0;36mCRNN1.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     47\u001B[0m x2 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout2(x2)\n\u001B[1;32m     48\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mconv2 output shape:\u001B[39m\u001B[38;5;124m\"\u001B[39m, x2\u001B[38;5;241m.\u001B[39mshape)\n\u001B[0;32m---> 50\u001B[0m x3 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39melu3(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbn3(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv3\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx2\u001B[49m\u001B[43m)\u001B[49m))\n\u001B[1;32m     51\u001B[0m x3 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout3(x3)\n\u001B[1;32m     52\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mconv3 output shape:\u001B[39m\u001B[38;5;124m\"\u001B[39m, x3\u001B[38;5;241m.\u001B[39mshape)\n",
      "File \u001B[0;32m~/PycharmProjects/Tagging-Music-Sequences/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/Tagging-Music-Sequences/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/Tagging-Music-Sequences/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py:460\u001B[0m, in \u001B[0;36mConv2d.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    459\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 460\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_conv_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/Tagging-Music-Sequences/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py:456\u001B[0m, in \u001B[0;36mConv2d._conv_forward\u001B[0;34m(self, input, weight, bias)\u001B[0m\n\u001B[1;32m    452\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mzeros\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[1;32m    453\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mconv2d(F\u001B[38;5;241m.\u001B[39mpad(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reversed_padding_repeated_twice, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode),\n\u001B[1;32m    454\u001B[0m                     weight, bias, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstride,\n\u001B[1;32m    455\u001B[0m                     _pair(\u001B[38;5;241m0\u001B[39m), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdilation, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgroups)\n\u001B[0;32m--> 456\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv2d\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    457\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdilation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgroups\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Run training\n",
    "trainer.train(epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd442e8d26be418",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T19:34:48.367399Z",
     "start_time": "2023-11-19T19:33:21.014543Z"
    },
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "trainer.evaluate(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10112a96aaac29d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T19:34:48.412911Z",
     "start_time": "2023-11-19T19:34:48.363722Z"
    },
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Save model\n",
    "path = '../models/spec_crnn.pth'\n",
    "trainer.save_model(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356c42f44bc7024a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T17:58:31.049965Z",
     "start_time": "2023-11-19T17:58:31.008467Z"
    },
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Load a model\n",
    "# path = '../models/MODELNAME.pth'\n",
    "# trainer.load_model(path)\n",
    "# print(trainer.history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}