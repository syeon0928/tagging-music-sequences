{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20a9a70d4fab1c1b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T10:06:39.151062Z",
     "start_time": "2023-11-16T10:06:39.026115Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/seuh/Tagging-Music-Sequences\n",
      "/home/seuh/Tagging-Music-Sequences/data/\n"
     ]
    }
   ],
   "source": [
    "# Set path variables\n",
    "import os\n",
    "import sys\n",
    "\n",
    "cwd = os.getcwd()\n",
    "project_dir = os.path.abspath(os.path.join(cwd, os.pardir))\n",
    "sys.path.append(project_dir)\n",
    "data_path = os.path.join(project_dir, 'data/')\n",
    "print(project_dir)\n",
    "print(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T10:06:42.180706Z",
     "start_time": "2023-11-16T10:06:39.026244Z"
    }
   },
   "outputs": [],
   "source": [
    "# for data loading process\n",
    "from src.data_loader import *\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "011ec918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262e806f361e2397",
   "metadata": {},
   "source": [
    "# CNN+Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4398c382dda8d981",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "872927e61b21ef60",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T10:06:42.482600Z",
     "start_time": "2023-11-16T10:06:42.183254Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load label annotation csv\n",
    "train_annotations = pd.read_csv(data_path + 'mtat_train_label.csv', index_col=0).reset_index(drop=True)\n",
    "val_annotations = pd.read_csv(data_path + 'mtat_val_label.csv', index_col=0).reset_index(drop=True)\n",
    "test_annotations = pd.read_csv(data_path + 'mtat_test_label.csv', index_col=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21576f271a1d3d6a",
   "metadata": {},
   "source": [
    "### FOR RAW AUDIO DATA\n",
    "\n",
    "Set transformation parameter to None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9539a0ecea0bf2af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T10:06:42.585350Z",
     "start_time": "2023-11-16T10:06:42.483251Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define global parameters across all classes\n",
    "DATA_DIR = data_path\n",
    "SAMPLE_RATE = 16000\n",
    "DURATION_IN_SEC = 15\n",
    "\n",
    "train_data = AudioDS(annotations_file=train_annotations, \n",
    "                     data_dir=DATA_DIR, \n",
    "                     target_sample_rate=SAMPLE_RATE, \n",
    "                     target_length=DURATION_IN_SEC, \n",
    "                     transformation=None)\n",
    "\n",
    "val_data = AudioDS(annotations_file=val_annotations,\n",
    "                     data_dir=DATA_DIR,\n",
    "                     target_sample_rate=SAMPLE_RATE,\n",
    "                     target_length=DURATION_IN_SEC,\n",
    "                     transformation=None)\n",
    "\n",
    "test_data = AudioDS(annotations_file=val_annotations,\n",
    "                     data_dir=DATA_DIR,\n",
    "                     target_sample_rate=SAMPLE_RATE,\n",
    "                     target_length=DURATION_IN_SEC,\n",
    "                     transformation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f5a368079a2380f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T10:06:42.590986Z",
     "start_time": "2023-11-16T10:06:42.586953Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load data from created datasets\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataloader = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_dataloader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c3dbdf0dee5b8391",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T10:06:43.846840Z",
     "start_time": "2023-11-16T10:06:42.591814Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([64, 1, 240000])\n",
      "Labels batch shape: torch.Size([64, 50])\n"
     ]
    }
   ],
   "source": [
    "# Display batch information\n",
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6a1023cfb21715c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T10:06:43.856377Z",
     "start_time": "2023-11-16T10:06:43.847197Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio file path: /home/seuh/Tagging-Music-Sequences/data/mtat/0/american_bach_soloists-joseph_haydn__masses-04-quoniam_tu_solus__allegro-30-59.mp3\n",
      "Label: tensor([ True, False, False, False, False,  True, False, False, False,  True,\n",
      "        False,  True, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False,  True,\n",
      "        False, False, False, False, False, False, False, False, False, False])\n",
      "Decoded labels: ['guitar', 'vocal', 'rock', 'male', 'bass']\n"
     ]
    }
   ],
   "source": [
    "# Retrieve a sample\n",
    "idx = 9\n",
    "waveform = train_features[idx]\n",
    "label = train_labels[idx]\n",
    "decoded_labels = train_data.decode_labels(label)\n",
    "file_path = train_data.get_filepath(idx)\n",
    "\n",
    "print(f\"Audio file path: {file_path}\")\n",
    "print(f\"Label: {label}\")\n",
    "print(f\"Decoded labels: {decoded_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "543eb7198521fa72",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T10:06:43.865530Z",
     "start_time": "2023-11-16T10:06:43.859170Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 240000])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape of waveform\n",
    "# first element: number of channels in our case 1\n",
    "# second element: number of samples in 30 seconds audio at a sampling rate of 16000 samples/s \n",
    "# -> 480000 = 30s * 16000 samples/s\n",
    "waveform.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a58dc0",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e71a58",
   "metadata": {},
   "source": [
    "### Front-end CNN+waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "693ef287",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels):\n",
    "        super(Conv1, self).__init__()\n",
    "        self.conv = nn.Conv1d(input_channels, output_channels, 1)\n",
    "        self.bn = nn.BatchNorm1d(output_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "    def forward(self, x):\n",
    "        out = self.relu(self.bn(self.conv(x)))\n",
    "        return out\n",
    "\n",
    "\n",
    "class Conv7(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels):\n",
    "        super(Conv7, self).__init__()\n",
    "        self.conv = nn.Conv1d(input_channels, output_channels, 7, padding=3)\n",
    "        self.bn = nn.BatchNorm1d(output_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.relu(self.bn(self.conv(x)))\n",
    "        return out\n",
    "\n",
    "\n",
    "class Conv3(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels):\n",
    "        super(Conv3, self).__init__()\n",
    "        self.conv = nn.Conv1d(input_channels, output_channels, 3, padding=1)\n",
    "        self.bn = nn.BatchNorm1d(output_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.mp = nn.MaxPool1d(3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.mp(self.relu(self.bn(self.conv(x))))\n",
    "        return out\n",
    "\n",
    "\n",
    "class InitConv(nn.Module):\n",
    "    def __init__(self, output_channels):\n",
    "        super(InitConv, self).__init__()\n",
    "        self.conv = nn.Conv1d(1, output_channels, 3, stride=3, padding=1)\n",
    "        self.bn = nn.BatchNorm1d(output_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        out = self.relu(self.bn(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "932a3e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeeConvModule(nn.Module):\n",
    "    def __init__(self, conv_channels, num_classes):\n",
    "        super(LeeConvModule, self).__init__()\n",
    "        \n",
    "        # initial convolution\n",
    "        self.init_conv = InitConv(conv_channels)\n",
    "\n",
    "        # stack convolution\n",
    "        c = conv_channels\n",
    "        channels = [c, c, c, c*2, c*2, c*2, c*2, c*2, c*4]\n",
    "        self.convs = nn.ModuleList([Conv3(channels[i], channels[i+1]) for i in range(len(channels)-1)])\n",
    "\n",
    "        # Adding three Conv7 layers\n",
    "        self.conv7x1_1 = Conv7(channels[-1], channels[-1])\n",
    "        \n",
    "        # Final classification layer\n",
    "        self.classifier = nn.Linear(channels[-1], num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.init_conv(x)\n",
    "        for layer in self.convs:\n",
    "            out = layer(out)\n",
    "            \n",
    "#         # Pass through Conv7 layers\n",
    "#         out = self.conv7x1_1(out)\n",
    "                                     \n",
    "        # Global average pooling before the classification layer\n",
    "        out = torch.mean(out, dim=-1)\n",
    "\n",
    "        # Apply the final classifier\n",
    "        logits = self.classifier(out)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b1c8a247",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainLeeConvModule:\n",
    "    def __init__(self, conv_channel, num_classes=50, lr=0.001, epochs=10, model_save_path='../models/model.pth', use_cuda=True):\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.model_save_path = model_save_path\n",
    "        self.use_cuda = use_cuda and torch.cuda.is_available()\n",
    "        \n",
    "        # Initialize the model\n",
    "        self.model = LeeConvModule(conv_channel, num_classes)\n",
    "        if self.use_cuda:\n",
    "            self.model.cuda()\n",
    "\n",
    "        # Loss function and optimizer\n",
    "        self.criterion = nn.BCELoss()\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "\n",
    "    def save_checkpoint(self, epoch):\n",
    "        checkpoint_path = f'{self.model_save_path}_epoch_{epoch}.pth'\n",
    "        torch.save(self.model.state_dict(), checkpoint_path)\n",
    "        print(f'Model saved to {checkpoint_path}')\n",
    "\n",
    "    def train(self, train_features, train_labels):\n",
    "        num_batches = train_features.shape[0]\n",
    "        # Iterate over batches with tqdm for progress display\n",
    "        tqdm_bar = tqdm(range(self.epochs), desc=f'Training Progress', leave=True)\n",
    "        for epoch in tqdm_bar:\n",
    "            self.model.train()\n",
    "            running_loss = 0.0\n",
    "\n",
    "            if self.use_cuda:\n",
    "                    train_features, train_labels = train_features.cuda(), train_labels.cuda()\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = self.model(train_features)\n",
    "            loss = self.criterion(outputs, train_labels)\n",
    "\n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            avg_loss = running_loss / len(train_features)\n",
    "            # Update tqdm bar\n",
    "            tqdm_bar.set_postfix(loss=avg_loss)\n",
    "\n",
    "        \n",
    "            # Save model checkpoint\n",
    "            self.save_checkpoint(epoch+1)\n",
    "\n",
    "            # Print average loss at the end of the epoch\n",
    "            avg_loss = running_loss / num_batches\n",
    "            print(f'Epoch [{epoch+1}/{self.epochs}] completed, Average Loss: {avg_loss:.4f}')\n",
    "\n",
    "        # Save the final model\n",
    "        torch.save(self.model.state_dict(), self.model_save_path)\n",
    "        print(f'Final model saved to {self.model_save_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0de0a10a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Training Progress:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacty of 7.79 GiB of which 21.25 MiB is free. Including non-PyTorch memory, this process has 7.76 GiB memory in use. Of the allocated memory 6.76 GiB is allocated by PyTorch, and 10.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 11\u001b[0m\n\u001b[1;32m      2\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconv_channel\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m64\u001b[39m,\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_classes\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m50\u001b[39m,   \u001b[38;5;66;03m# As per your requirement\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_save_path\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../models/wave_conv_model.pth\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;66;03m# Path to save the model\u001b[39;00m\n\u001b[1;32m      8\u001b[0m }\n\u001b[1;32m     10\u001b[0m trainer \u001b[38;5;241m=\u001b[39m TrainLeeConvModule(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig)\n\u001b[0;32m---> 11\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain(train_features, train_labels)\n",
      "Cell \u001b[0;32mIn[57], line 31\u001b[0m, in \u001b[0;36mTrainLeeConvModule.train\u001b[0;34m(self, train_features, train_labels)\u001b[0m\n\u001b[1;32m     28\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_cuda:\n\u001b[0;32m---> 31\u001b[0m         train_features, train_labels \u001b[38;5;241m=\u001b[39m train_features\u001b[38;5;241m.\u001b[39mcuda(), train_labels\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Zero the parameter gradients\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacty of 7.79 GiB of which 21.25 MiB is free. Including non-PyTorch memory, this process has 7.76 GiB memory in use. Of the allocated memory 6.76 GiB is allocated by PyTorch, and 10.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize training\n",
    "config = {\n",
    "    'conv_channel':64,\n",
    "    'num_classes': 50,   # As per your requirement\n",
    "    'lr': 0.001,         # Learning rate\n",
    "    'epochs': 10,        # Number of epochs\n",
    "    'model_save_path': '../models/wave_conv_model.pth', # Path to save the model\n",
    "}\n",
    "\n",
    "trainer = TrainLeeConvModule(**config)\n",
    "trainer.train(train_features, train_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fde42ef",
   "metadata": {},
   "source": [
    "### different architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7a88b63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class WaveformNet(nn.Module):\n",
    "    def __init__(self, num_classes=50):\n",
    "        super(WaveformNet, self).__init__()\n",
    "        \n",
    "        # Strided convolution to reduce dimensionality\n",
    "        self.strided_conv = nn.Conv1d(1, 128, kernel_size=3, stride=3, padding=1)\n",
    "        self.bn0 = nn.BatchNorm1d(128)\n",
    "\n",
    "        # Convolutional blocks\n",
    "        self.conv_blocks = nn.ModuleList()\n",
    "        for i in range(9):\n",
    "            out_channels = 128 * (2 if i > 0 else 1)  # Double the channels after the first block\n",
    "            self.conv_blocks.append(nn.Conv1d(128, out_channels, kernel_size=3, padding=1))\n",
    "            self.conv_blocks.append(nn.BatchNorm1d(out_channels))\n",
    "            self.conv_blocks.append(nn.ReLU())\n",
    "            self.conv_blocks.append(nn.MaxPool1d(kernel_size=3, stride=3))\n",
    "\n",
    "        # Global max pooling\n",
    "        self.global_max_pool = nn.AdaptiveMaxPool1d(1)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(512, 256)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initial strided convolution\n",
    "        x = F.relu(self.bn0(self.strided_conv(x)))\n",
    "\n",
    "        # Convolutional blocks\n",
    "        for block in self.conv_blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        # Global max pooling\n",
    "        x = self.global_max_pool(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1a298e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "\n",
    "class TrainWaveformNet:\n",
    "    def __init__(self, num_classes, learning_rate, epochs, model_save_path, use_cuda):\n",
    "        self.epochs = epochs\n",
    "        self.model_save_path = model_save_path\n",
    "        self.use_cuda = use_cuda and torch.cuda.is_available()\n",
    "        self.device = torch.device(\"cuda\" if self.use_cuda else \"cpu\")\n",
    "        \n",
    "        # Initialize the model\n",
    "        self.model = WaveformNet(num_classes=num_classes)\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        # Loss function and optimizer\n",
    "        self.criterion = nn.BCEWithLogitsLoss()  # Using BCEWithLogitsLoss for stability\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "\n",
    "    def save_checkpoint(self, epoch):\n",
    "        # Save a checkpoint of the model\n",
    "        checkpoint_path = f'{self.model_save_path}_checkpoint_epoch_{epoch}.pth'\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "        }, checkpoint_path)\n",
    "        print(f'Checkpoint saved: {checkpoint_path}')\n",
    "\n",
    "    def train(self, train_features, train_labels):\n",
    "        # Split the dataset into smaller chunks if necessary\n",
    "        chunk_size = 8  # Determine a suitable chunk size based on your GPU memory\n",
    "        num_chunks = len(train_features) // chunk_size\n",
    "        \n",
    "        # Progress bar setup\n",
    "        pbar = tqdm(total=self.epochs * num_chunks, desc='Training', leave=True)\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            self.model.train()\n",
    "            running_loss = 0.0\n",
    "            \n",
    "            for chunk in range(num_chunks):\n",
    "                # Get the current chunk of data\n",
    "                start_index = chunk * chunk_size\n",
    "                end_index = start_index + chunk_size\n",
    "                batch_features = train_features[start_index:end_index]\n",
    "                batch_labels = train_labels[start_index:end_index]\n",
    "                \n",
    "                batch_features, batch_labels = batch_features.to(self.device), batch_labels.to(self.device)\n",
    "\n",
    "                # Forward pass\n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.model(batch_features)\n",
    "                loss = self.criterion(outputs, batch_labels)\n",
    "                running_loss += loss.item()\n",
    "\n",
    "                # Backward and optimize\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                # Update progress bar\n",
    "                pbar.set_postfix({'epoch': epoch+1, 'loss': running_loss / (chunk + 1)})\n",
    "                pbar.update(1)\n",
    "\n",
    "            # Save checkpoint after each epoch\n",
    "            self.save_checkpoint(epoch)\n",
    "\n",
    "        # Save the final model\n",
    "        final_model_path = f'{self.model_save_path}_final.pth'\n",
    "        torch.save(self.model.state_dict(), final_model_path)\n",
    "        print(f'Final model saved to {final_model_path}')\n",
    "\n",
    "        # Close the progress bar\n",
    "        pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f5543104",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Training:   0%|          | 0/80 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 7.79 GiB of which 5.25 MiB is free. Including non-PyTorch memory, this process has 7.78 GiB memory in use. Of the allocated memory 6.78 GiB is allocated by PyTorch, and 11.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[74], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Assume train_features and train_labels are already tensors with the correct shape\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Example usage:\u001b[39;00m\n\u001b[1;32m      9\u001b[0m trainer \u001b[38;5;241m=\u001b[39m TrainWaveformNet(num_classes, learning_rate, epochs, model_save_path, use_cuda\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available())\n\u001b[0;32m---> 10\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain(train_features, train_labels)\n",
      "Cell \u001b[0;32mIn[73], line 48\u001b[0m, in \u001b[0;36mTrainWaveformNet.train\u001b[0;34m(self, train_features, train_labels)\u001b[0m\n\u001b[1;32m     45\u001b[0m batch_features \u001b[38;5;241m=\u001b[39m train_features[start_index:end_index]\n\u001b[1;32m     46\u001b[0m batch_labels \u001b[38;5;241m=\u001b[39m train_labels[start_index:end_index]\n\u001b[0;32m---> 48\u001b[0m batch_features, batch_labels \u001b[38;5;241m=\u001b[39m batch_features\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice), batch_labels\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 7.79 GiB of which 5.25 MiB is free. Including non-PyTorch memory, this process has 7.78 GiB memory in use. Of the allocated memory 6.78 GiB is allocated by PyTorch, and 11.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# Parameters for the model and training\n",
    "num_classes = 50\n",
    "learning_rate = 0.001\n",
    "epochs = 10\n",
    "model_save_path = '../models/waveform_model'  # Adjust the path as needed\n",
    "\n",
    "# Assume train_features and train_labels are already tensors with the correct shape\n",
    "# Example usage:\n",
    "trainer = TrainWaveformNet(num_classes, learning_rate, epochs, model_save_path, use_cuda=torch.cuda.is_available())\n",
    "trainer.train(train_features, train_labels)  # train_features and train_labels should be tensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7dd157f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b01453",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e28275",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
