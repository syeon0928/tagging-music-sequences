{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ab/Projects/Tagging-Music-Sequences\n",
      "/Users/ab/Projects/Tagging-Music-Sequences/data/\n"
     ]
    }
   ],
   "source": [
    "# Set path variables\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "cwd = os.getcwd()\n",
    "project_dir = os.path.abspath(os.path.join(cwd, os.pardir))\n",
    "sys.path.append(project_dir)\n",
    "data_path = os.path.join(project_dir, 'data/')\n",
    "print(project_dir)\n",
    "print(data_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T13:30:59.576769Z",
     "start_time": "2023-11-19T13:30:59.319694Z"
    }
   },
   "id": "aa5c8c4cd82421f4"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# for data loading process\n",
    "\n",
    "from src.audio_dataset import *\n",
    "from torch.utils.data import DataLoader\n",
    "from src.trainer import Trainer\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torchsummary import summary\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T13:31:03.726506Z",
     "start_time": "2023-11-19T13:30:59.327913Z"
    }
   },
   "id": "92539e7afd45cfda"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T13:31:03.765356Z",
     "start_time": "2023-11-19T13:31:03.756726Z"
    }
   },
   "id": "3e2cf8fd18c8c61a"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is not available. Please check your system's configuration.\n"
     ]
    }
   ],
   "source": [
    "# Make sure CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    device_name = torch.cuda.get_device_name(torch.cuda.current_device())\n",
    "    print(f\"Current CUDA Device Name: {device_name}\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Please check your system's configuration.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T13:31:03.782414Z",
     "start_time": "2023-11-19T13:31:03.764541Z"
    }
   },
   "id": "402651522b5fbecd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# CNN+Attention"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7f2acb435a82d11b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data loading"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "38891c1d977f660d"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Load label annotation csv\n",
    "train_annotations = 'mtat_train_label.csv'\n",
    "val_annotations = 'mtat_val_label.csv'\n",
    "test_annotations = 'mtat_test_label.csv'\n",
    "\n",
    "# data path\n",
    "cwd = Path.cwd()\n",
    "DATA_DIR = cwd.parent / 'data'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T13:31:03.782547Z",
     "start_time": "2023-11-19T13:31:03.771307Z"
    }
   },
   "id": "8a5ac67d3dded785"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### FOR RAW AUDIO DATA\n",
    "\n",
    "Set transformation parameter to None"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "be497231d208e7b3"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# Define global parameters across all classes\n",
    "SAMPLE_RATE = 16000\n",
    "DURATION_IN_SEC = 29.1\n",
    "\n",
    "train_data = AudioDS(annotations_file=train_annotations, \n",
    "                     data_dir=DATA_DIR, \n",
    "                     target_sample_rate=SAMPLE_RATE, \n",
    "                     target_length=DURATION_IN_SEC, \n",
    "                     transformation=None)\n",
    "\n",
    "val_data = AudioDS(annotations_file=val_annotations,\n",
    "                     data_dir=DATA_DIR,\n",
    "                     target_sample_rate=SAMPLE_RATE,\n",
    "                     target_length=DURATION_IN_SEC,\n",
    "                     transformation=None)\n",
    "\n",
    "test_data = AudioDS(annotations_file=val_annotations,\n",
    "                     data_dir=DATA_DIR,\n",
    "                     target_sample_rate=SAMPLE_RATE,\n",
    "                     target_length=DURATION_IN_SEC,\n",
    "                     transformation=None)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T13:31:04.106222Z",
     "start_time": "2023-11-19T13:31:03.779309Z"
    }
   },
   "id": "39d95e461181ad29"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Load data from created datasets\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataloader = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_dataloader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T13:31:04.130623Z",
     "start_time": "2023-11-19T13:31:04.107886Z"
    }
   },
   "id": "a3a103f8e90c3094"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "406\n",
      "136\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataloader))\n",
    "print(len(val_dataloader))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T13:31:04.131431Z",
     "start_time": "2023-11-19T13:31:04.111584Z"
    }
   },
   "id": "3f01af1b54dad5fc"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([32, 1, 465600])\n",
      "Labels batch shape: torch.Size([32, 50])\n"
     ]
    }
   ],
   "source": [
    "# Display batch information\n",
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T13:31:04.844198Z",
     "start_time": "2023-11-19T13:31:04.117014Z"
    }
   },
   "id": "93a965413b7f1aee"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio file path: /Users/ab/Projects/Tagging-Music-Sequences/data/mtat/0/paul_berget-the_siena_manuscript_on_steel_string_guitar-06-recercar_6_steel_string_guitar-30-59.mp3\n",
      "Label: tensor([0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       dtype=torch.float64)\n",
      "Decoded labels: ['classical', 'slow', 'piano', 'soft', 'solo']\n"
     ]
    }
   ],
   "source": [
    "# Retrieve a sample\n",
    "idx = 0\n",
    "waveform = train_features[idx]\n",
    "label = train_labels[idx]\n",
    "decoded_labels = train_data.decode_labels(label)\n",
    "file_path = train_data.get_filepath(idx)\n",
    "\n",
    "print(f\"Audio file path: {file_path}\")\n",
    "print(f\"Label: {label}\")\n",
    "print(f\"Decoded labels: {decoded_labels}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T13:31:04.857470Z",
     "start_time": "2023-11-19T13:31:04.844869Z"
    }
   },
   "id": "88b4ab5aac317eef"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 465600])"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape of waveform\n",
    "# first element: number of channels in our case 1\n",
    "# second element: number of samples in 30 seconds audio at a sampling rate of 16000 samples/s \n",
    "# -> 480000 = 30s * 16000 samples/s\n",
    "waveform.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T13:31:04.862814Z",
     "start_time": "2023-11-19T13:31:04.855004Z"
    }
   },
   "id": "f9ebc2dce8648e4a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Build Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d4156651706ee987"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Front-end CNN+waveform"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "de16859034fdd5c3"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "class WaveformNet(nn.Module):\n",
    "    def __init__(self, num_classes=50):\n",
    "        super(WaveformNet, self).__init__()\n",
    "        \n",
    "        # Strided convolution to reduce dimensionality\n",
    "        self.strided_conv = nn.Conv1d(1, 128, kernel_size=3, stride=3)\n",
    "        self.bn0 = nn.BatchNorm1d(128)\n",
    "\n",
    "        # Convolutional blocks\n",
    "        self.conv_blocks = nn.ModuleList()\n",
    "        in_channels = 128\n",
    "        out_channels = 128\n",
    "        for i in range(8):\n",
    "            \n",
    "            if i == 3:  # 4th layer\n",
    "                out_channels = 256\n",
    "            if i == 7:  # Last layer\n",
    "                out_channels = 512\n",
    "                \n",
    "            self.conv_blocks.append(nn.Conv1d(in_channels, out_channels, kernel_size=3))\n",
    "            self.conv_blocks.append(nn.BatchNorm1d(out_channels))\n",
    "            self.conv_blocks.append(nn.ReLU())\n",
    "            self.conv_blocks.append(nn.MaxPool1d(kernel_size=3, stride=3))\n",
    "            in_channels = out_channels  \n",
    "\n",
    "        # Global max pooling\n",
    "        self.global_max_pool = nn.AdaptiveMaxPool1d(1)\n",
    "\n",
    "        # Fully connected layers\n",
    "        # Adjust the input size of the first FC layer based on the output of the last convolutional block\n",
    "        self.fc1 = nn.Linear(512, 256)  # Adjust 128 based on the output channels of the last conv block\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initial strided convolution\n",
    "        x = F.relu(self.bn0(self.strided_conv(x)))\n",
    "\n",
    "        # Convolutional blocks\n",
    "        for block in self.conv_blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        # Global max pooling\n",
    "        x = self.global_max_pool(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T13:31:04.934066Z",
     "start_time": "2023-11-19T13:31:04.864497Z"
    }
   },
   "id": "b1d226ace81cb27d"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv1d-1          [-1, 128, 155200]             512\n",
      "       BatchNorm1d-2          [-1, 128, 155200]             256\n",
      "            Conv1d-3          [-1, 128, 155198]          49,280\n",
      "       BatchNorm1d-4          [-1, 128, 155198]             256\n",
      "              ReLU-5          [-1, 128, 155198]               0\n",
      "         MaxPool1d-6           [-1, 128, 51732]               0\n",
      "            Conv1d-7           [-1, 128, 51730]          49,280\n",
      "       BatchNorm1d-8           [-1, 128, 51730]             256\n",
      "              ReLU-9           [-1, 128, 51730]               0\n",
      "        MaxPool1d-10           [-1, 128, 17243]               0\n",
      "           Conv1d-11           [-1, 128, 17241]          49,280\n",
      "      BatchNorm1d-12           [-1, 128, 17241]             256\n",
      "             ReLU-13           [-1, 128, 17241]               0\n",
      "        MaxPool1d-14            [-1, 128, 5747]               0\n",
      "           Conv1d-15            [-1, 256, 5745]          98,560\n",
      "      BatchNorm1d-16            [-1, 256, 5745]             512\n",
      "             ReLU-17            [-1, 256, 5745]               0\n",
      "        MaxPool1d-18            [-1, 256, 1915]               0\n",
      "           Conv1d-19            [-1, 256, 1913]         196,864\n",
      "      BatchNorm1d-20            [-1, 256, 1913]             512\n",
      "             ReLU-21            [-1, 256, 1913]               0\n",
      "        MaxPool1d-22             [-1, 256, 637]               0\n",
      "           Conv1d-23             [-1, 256, 635]         196,864\n",
      "      BatchNorm1d-24             [-1, 256, 635]             512\n",
      "             ReLU-25             [-1, 256, 635]               0\n",
      "        MaxPool1d-26             [-1, 256, 211]               0\n",
      "           Conv1d-27             [-1, 256, 209]         196,864\n",
      "      BatchNorm1d-28             [-1, 256, 209]             512\n",
      "             ReLU-29             [-1, 256, 209]               0\n",
      "        MaxPool1d-30              [-1, 256, 69]               0\n",
      "           Conv1d-31              [-1, 512, 67]         393,728\n",
      "      BatchNorm1d-32              [-1, 512, 67]           1,024\n",
      "             ReLU-33              [-1, 512, 67]               0\n",
      "        MaxPool1d-34              [-1, 512, 22]               0\n",
      "AdaptiveMaxPool1d-35               [-1, 512, 1]               0\n",
      "           Linear-36                  [-1, 256]         131,328\n",
      "           Linear-37                   [-1, 50]          12,850\n",
      "================================================================\n",
      "Total params: 1,379,506\n",
      "Trainable params: 1,379,506\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 1.78\n",
      "Forward/backward pass size (MB): 1089.07\n",
      "Params size (MB): 5.26\n",
      "Estimated Total Size (MB): 1096.10\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = WaveformNet(num_classes=50)\n",
    "input_size = (train_features.size()[1:])  \n",
    "summary(model, input_size)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T13:31:10.854334Z",
     "start_time": "2023-11-19T13:31:07.897467Z"
    }
   },
   "id": "505bc3bae902d6e8"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# Instantiate trainer\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 10\n",
    "num_classes = 50\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# train model\n",
    "trainer = Trainer(model, train_dataloader, val_dataloader, criterion, optimizer, device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T13:31:32.981520Z",
     "start_time": "2023-11-19T13:31:31.962634Z"
    }
   },
   "id": "c9798a2c09b877be"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training:   0%|          | 0/10 [00:00<?, ?it/s]\u001B[A"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[30], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mEPOCHS\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Projects/Tagging-Music-Sequences/src/trainer.py:28\u001B[0m, in \u001B[0;36mTrainer.train\u001B[0;34m(self, epochs)\u001B[0m\n\u001B[1;32m     25\u001B[0m inputs, labels \u001B[38;5;241m=\u001B[39m inputs\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice), labels\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[1;32m     27\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m---> 28\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     29\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcriterion(outputs, labels)\n\u001B[1;32m     30\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n",
      "File \u001B[0;32m~/Projects/Tagging-Music-Sequences/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Projects/Tagging-Music-Sequences/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[27], line 40\u001B[0m, in \u001B[0;36mWaveformNet.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     38\u001B[0m \u001B[38;5;66;03m# Convolutional blocks\u001B[39;00m\n\u001B[1;32m     39\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m block \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconv_blocks:\n\u001B[0;32m---> 40\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[43mblock\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     42\u001B[0m \u001B[38;5;66;03m# Global max pooling\u001B[39;00m\n\u001B[1;32m     43\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mglobal_max_pool(x)\n",
      "File \u001B[0;32m~/Projects/Tagging-Music-Sequences/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Projects/Tagging-Music-Sequences/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/Projects/Tagging-Music-Sequences/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py:310\u001B[0m, in \u001B[0;36mConv1d.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    309\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 310\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_conv_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Projects/Tagging-Music-Sequences/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py:306\u001B[0m, in \u001B[0;36mConv1d._conv_forward\u001B[0;34m(self, input, weight, bias)\u001B[0m\n\u001B[1;32m    302\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mzeros\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[1;32m    303\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mconv1d(F\u001B[38;5;241m.\u001B[39mpad(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reversed_padding_repeated_twice, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode),\n\u001B[1;32m    304\u001B[0m                     weight, bias, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstride,\n\u001B[1;32m    305\u001B[0m                     _single(\u001B[38;5;241m0\u001B[39m), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdilation, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgroups)\n\u001B[0;32m--> 306\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv1d\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    307\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdilation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgroups\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train(epochs=EPOCHS)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T13:10:37.555563Z",
     "start_time": "2023-11-19T13:07:19.010163Z"
    }
   },
   "id": "83bf7ff8e6e4c85e"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "trainer.save_model('../models/waveform_cnn.pth')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T13:31:44.147256Z",
     "start_time": "2023-11-19T13:31:43.962346Z"
    }
   },
   "id": "ee6e5b76e3bd762a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
