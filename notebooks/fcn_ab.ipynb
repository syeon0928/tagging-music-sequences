{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-19T13:29:59.733526Z",
     "start_time": "2023-11-19T13:29:55.278948Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from src.audio_dataset import AudioDS\n",
    "from src.audio_util import *\n",
    "from src.trainer import Trainer\n",
    "from src.model_alex import FullyConvNet4, FullyConvNet5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "False"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set device to GPU if possible\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.is_available()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T13:29:59.747485Z",
     "start_time": "2023-11-19T13:29:59.739021Z"
    }
   },
   "id": "dd909e4f0f3729d6"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Load label annotation csv\n",
    "train_annotations = 'mtat_train_label.csv'\n",
    "val_annotations = 'mtat_val_label.csv'\n",
    "test_annotations = 'mtat_test_label.csv'\n",
    "\n",
    "# Data path\n",
    "from pathlib import Path\n",
    "cwd = Path.cwd()\n",
    "DATA_DIR = cwd.parent / 'data'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T13:29:59.751883Z",
     "start_time": "2023-11-19T13:29:59.746349Z"
    }
   },
   "id": "c1d4b919e451b545"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Transformations on dataset\n",
    "SAMPLE_RATE = 16000\n",
    "DURATION_IN_SEC = 29.1\n",
    "MEL_SPEC_DB_TRANSFORMATION = AudioUtil.get_audio_transforms(SAMPLE_RATE,\n",
    "                                                            n_fft=512,\n",
    "                                                            hop_length=256,\n",
    "                                                            n_mels=96,\n",
    "                                                            top_db=80)\n",
    "\n",
    "train_data = AudioDS(annotations_file=train_annotations,\n",
    "                     data_dir=DATA_DIR,\n",
    "                     target_sample_rate=SAMPLE_RATE,\n",
    "                     target_length=DURATION_IN_SEC,\n",
    "                     transformation=MEL_SPEC_DB_TRANSFORMATION)\n",
    "\n",
    "val_data = AudioDS(annotations_file=val_annotations,\n",
    "                   data_dir=DATA_DIR,\n",
    "                   target_sample_rate=SAMPLE_RATE,\n",
    "                   target_length=DURATION_IN_SEC,\n",
    "                   transformation=MEL_SPEC_DB_TRANSFORMATION)\n",
    "\n",
    "test_data = AudioDS(annotations_file=val_annotations,\n",
    "                    data_dir=DATA_DIR,\n",
    "                    target_sample_rate=SAMPLE_RATE,\n",
    "                    target_length=DURATION_IN_SEC,\n",
    "                    transformation=MEL_SPEC_DB_TRANSFORMATION)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T13:29:59.972408Z",
     "start_time": "2023-11-19T13:29:59.753576Z"
    }
   },
   "id": "3a51947bfa0d5ffe"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 10"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T13:29:59.986959Z",
     "start_time": "2023-11-19T13:29:59.974802Z"
    }
   },
   "id": "192e035be5eaab94"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataloader = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_dataloader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T13:29:59.987350Z",
     "start_time": "2023-11-19T13:29:59.981006Z"
    }
   },
   "id": "ad2d909def5cd005"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([32, 1, 96, 1819])\n",
      "Labels batch shape: torch.Size([32, 50])\n"
     ]
    }
   ],
   "source": [
    "# Display batch information\n",
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T13:30:00.802712Z",
     "start_time": "2023-11-19T13:29:59.985840Z"
    }
   },
   "id": "8ce2d4043561fffa"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "### TEST BLOCK (SMALL DATASET)\n",
    "# Hyperparameters for subset\n",
    "SUBSET_SIZE = 100  # Adjust this number based on your requirement\n",
    "\n",
    "# Create random indices for train and validation subsets\n",
    "train_indices = np.random.choice(len(train_data), SUBSET_SIZE, replace=False)\n",
    "val_indices = np.random.choice(len(val_data), SUBSET_SIZE, replace=False)\n",
    "\n",
    "# Create subsets\n",
    "train_subset = Subset(train_data, train_indices)\n",
    "val_subset = Subset(val_data, val_indices)\n",
    "\n",
    "# Create dataloaders for subsets\n",
    "train_dataloader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataloader = DataLoader(val_subset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "### TEST END"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T13:30:00.815044Z",
     "start_time": "2023-11-19T13:30:00.804552Z"
    }
   },
   "id": "1f00dfe844e7f063"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# Instantiate model\n",
    "fcn4 = FullyConvNet4()\n",
    "fcn5 = FullyConvNet5()\n",
    "\n",
    "# Instantiate trainer\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(fcn4.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "trainer = Trainer(fcn4, train_dataloader, val_dataloader, criterion, optimizer, device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T13:30:01.415953Z",
     "start_time": "2023-11-19T13:30:00.811105Z"
    }
   },
   "id": "2b489b3b45994717"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 96, 1819])\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 16, 96, 1819]             160\n",
      "         MaxPool2d-2          [-1, 16, 48, 454]               0\n",
      "            Conv2d-3          [-1, 32, 48, 454]           4,640\n",
      "         MaxPool2d-4           [-1, 32, 12, 90]               0\n",
      "            Conv2d-5           [-1, 64, 12, 90]          18,496\n",
      "         MaxPool2d-6            [-1, 64, 4, 11]               0\n",
      "            Conv2d-7           [-1, 128, 4, 11]          73,856\n",
      "         MaxPool2d-8            [-1, 128, 1, 1]               0\n",
      "            Conv2d-9             [-1, 50, 1, 1]           6,450\n",
      "================================================================\n",
      "Total params: 103,602\n",
      "Trainable params: 103,602\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.67\n",
      "Forward/backward pass size (MB): 30.15\n",
      "Params size (MB): 0.40\n",
      "Estimated Total Size (MB): 31.22\n",
      "----------------------------------------------------------------\n",
      "None\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1        [-1, 128, 96, 1819]           1,280\n",
      "         MaxPool2d-2         [-1, 128, 48, 454]               0\n",
      "            Conv2d-3         [-1, 256, 48, 454]         295,168\n",
      "         MaxPool2d-4         [-1, 256, 24, 113]               0\n",
      "            Conv2d-5         [-1, 512, 24, 113]       1,180,160\n",
      "         MaxPool2d-6          [-1, 512, 12, 28]               0\n",
      "            Conv2d-7         [-1, 1024, 12, 28]       4,719,616\n",
      "         MaxPool2d-8           [-1, 1024, 4, 5]               0\n",
      "            Conv2d-9           [-1, 2048, 4, 5]      18,876,416\n",
      "        MaxPool2d-10           [-1, 2048, 1, 1]               0\n",
      "           Conv2d-11             [-1, 50, 1, 1]         102,450\n",
      "================================================================\n",
      "Total params: 25,175,090\n",
      "Trainable params: 25,175,090\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.67\n",
      "Forward/backward pass size (MB): 254.69\n",
      "Params size (MB): 96.04\n",
      "Estimated Total Size (MB): 351.39\n",
      "----------------------------------------------------------------\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "input_size = (train_features.size()[1:])\n",
    "print(input_size)\n",
    "print(summary(fcn4, input_size))\n",
    "print(summary(fcn5, input_size))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T13:30:02.729603Z",
     "start_time": "2023-11-19T13:30:01.420814Z"
    }
   },
   "id": "6e56e47c41c569f9"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  10%|â–ˆ         | 1/10 [00:11<01:47, 11.94s/it, epoch=1, loss=0.13]"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Only one class present in y_true. ROC AUC score is not defined in that case.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[11], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Run training\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mEPOCHS\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Projects/Tagging-Music-Sequences/src/trainer.py:43\u001B[0m, in \u001B[0;36mTrainer.train\u001B[0;34m(self, epochs)\u001B[0m\n\u001B[1;32m     40\u001B[0m pbar\u001B[38;5;241m.\u001B[39mupdate(\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m     42\u001B[0m \u001B[38;5;66;03m# Retrieve values from validation phase in current epoch\u001B[39;00m\n\u001B[0;32m---> 43\u001B[0m val_loss, val_accuracy, val_roc_auc, val_pr_auc \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalidate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mepoch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     44\u001B[0m history[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mval_loss\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mappend(val_loss)\n\u001B[1;32m     45\u001B[0m history[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mval_accuracy\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mappend(val_accuracy)\n",
      "File \u001B[0;32m~/Projects/Tagging-Music-Sequences/src/trainer.py:91\u001B[0m, in \u001B[0;36mTrainer.validate\u001B[0;34m(self, epoch)\u001B[0m\n\u001B[1;32m     87\u001B[0m accuracy \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mmean(np\u001B[38;5;241m.\u001B[39mall(predicted \u001B[38;5;241m==\u001B[39m all_labels, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m))\n\u001B[1;32m     89\u001B[0m \u001B[38;5;66;03m# Calculate performance metrics\u001B[39;00m\n\u001B[1;32m     90\u001B[0m \u001B[38;5;66;03m# use ONE vs ONE approach for ROC AUC as papers suggest to handle class imbalance better\u001B[39;00m\n\u001B[0;32m---> 91\u001B[0m roc_auc \u001B[38;5;241m=\u001B[39m \u001B[43mroc_auc_score\u001B[49m\u001B[43m(\u001B[49m\u001B[43mall_labels\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mall_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maverage\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mmacro\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmulti_class\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43movo\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     92\u001B[0m pr_auc \u001B[38;5;241m=\u001B[39m average_precision_score(all_labels, all_outputs, average\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmacro\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     93\u001B[0m precision, recall, _ \u001B[38;5;241m=\u001B[39m precision_recall_curve(all_labels\u001B[38;5;241m.\u001B[39mravel(), all_outputs\u001B[38;5;241m.\u001B[39mravel())\n",
      "File \u001B[0;32m~/Projects/Tagging-Music-Sequences/venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:214\u001B[0m, in \u001B[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    208\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    209\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[1;32m    210\u001B[0m         skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[1;32m    211\u001B[0m             prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[1;32m    212\u001B[0m         )\n\u001B[1;32m    213\u001B[0m     ):\n\u001B[0;32m--> 214\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    215\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m InvalidParameterError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    216\u001B[0m     \u001B[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001B[39;00m\n\u001B[1;32m    217\u001B[0m     \u001B[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001B[39;00m\n\u001B[1;32m    218\u001B[0m     \u001B[38;5;66;03m# the name of the estimator by the name of the function in the error\u001B[39;00m\n\u001B[1;32m    219\u001B[0m     \u001B[38;5;66;03m# message to avoid confusion.\u001B[39;00m\n\u001B[1;32m    220\u001B[0m     msg \u001B[38;5;241m=\u001B[39m re\u001B[38;5;241m.\u001B[39msub(\n\u001B[1;32m    221\u001B[0m         \u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameter of \u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mw+ must be\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    222\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameter of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__qualname__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must be\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    223\u001B[0m         \u001B[38;5;28mstr\u001B[39m(e),\n\u001B[1;32m    224\u001B[0m     )\n",
      "File \u001B[0;32m~/Projects/Tagging-Music-Sequences/venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py:635\u001B[0m, in \u001B[0;36mroc_auc_score\u001B[0;34m(y_true, y_score, average, sample_weight, max_fpr, multi_class, labels)\u001B[0m\n\u001B[1;32m    627\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _average_binary_score(\n\u001B[1;32m    628\u001B[0m         partial(_binary_roc_auc_score, max_fpr\u001B[38;5;241m=\u001B[39mmax_fpr),\n\u001B[1;32m    629\u001B[0m         y_true,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    632\u001B[0m         sample_weight\u001B[38;5;241m=\u001B[39msample_weight,\n\u001B[1;32m    633\u001B[0m     )\n\u001B[1;32m    634\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:  \u001B[38;5;66;03m# multilabel-indicator\u001B[39;00m\n\u001B[0;32m--> 635\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_average_binary_score\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    636\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpartial\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_binary_roc_auc_score\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_fpr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_fpr\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    637\u001B[0m \u001B[43m        \u001B[49m\u001B[43my_true\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    638\u001B[0m \u001B[43m        \u001B[49m\u001B[43my_score\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    639\u001B[0m \u001B[43m        \u001B[49m\u001B[43maverage\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    640\u001B[0m \u001B[43m        \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msample_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    641\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Projects/Tagging-Music-Sequences/venv/lib/python3.11/site-packages/sklearn/metrics/_base.py:118\u001B[0m, in \u001B[0;36m_average_binary_score\u001B[0;34m(binary_metric, y_true, y_score, average, sample_weight)\u001B[0m\n\u001B[1;32m    116\u001B[0m     y_true_c \u001B[38;5;241m=\u001B[39m y_true\u001B[38;5;241m.\u001B[39mtake([c], axis\u001B[38;5;241m=\u001B[39mnot_average_axis)\u001B[38;5;241m.\u001B[39mravel()\n\u001B[1;32m    117\u001B[0m     y_score_c \u001B[38;5;241m=\u001B[39m y_score\u001B[38;5;241m.\u001B[39mtake([c], axis\u001B[38;5;241m=\u001B[39mnot_average_axis)\u001B[38;5;241m.\u001B[39mravel()\n\u001B[0;32m--> 118\u001B[0m     score[c] \u001B[38;5;241m=\u001B[39m \u001B[43mbinary_metric\u001B[49m\u001B[43m(\u001B[49m\u001B[43my_true_c\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_score_c\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mscore_weight\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    120\u001B[0m \u001B[38;5;66;03m# Average the results\u001B[39;00m\n\u001B[1;32m    121\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m average \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m~/Projects/Tagging-Music-Sequences/venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py:382\u001B[0m, in \u001B[0;36m_binary_roc_auc_score\u001B[0;34m(y_true, y_score, sample_weight, max_fpr)\u001B[0m\n\u001B[1;32m    380\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Binary roc auc score.\"\"\"\u001B[39;00m\n\u001B[1;32m    381\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(np\u001B[38;5;241m.\u001B[39munique(y_true)) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m2\u001B[39m:\n\u001B[0;32m--> 382\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    383\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOnly one class present in y_true. ROC AUC score \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    384\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mis not defined in that case.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    385\u001B[0m     )\n\u001B[1;32m    387\u001B[0m fpr, tpr, _ \u001B[38;5;241m=\u001B[39m roc_curve(y_true, y_score, sample_weight\u001B[38;5;241m=\u001B[39msample_weight)\n\u001B[1;32m    388\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m max_fpr \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m max_fpr \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n",
      "\u001B[0;31mValueError\u001B[0m: Only one class present in y_true. ROC AUC score is not defined in that case."
     ]
    }
   ],
   "source": [
    "# Run training\n",
    "trainer.train(epochs=EPOCHS)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T13:30:21.268574Z",
     "start_time": "2023-11-19T13:30:02.730865Z"
    }
   },
   "id": "51faac7a19a03e31"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-19T13:30:21.271321Z"
    }
   },
   "id": "10112a96aaac29d1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
