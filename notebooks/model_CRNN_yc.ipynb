{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20a9a70d4fab1c1b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T10:06:39.151062Z",
     "start_time": "2023-11-16T10:06:39.026115Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/bonuobonuo/Documents/GitHub/Tagging-Music-Sequences\n",
      "/Users/bonuobonuo/Documents/GitHub/Tagging-Music-Sequences/data/\n"
     ]
    }
   ],
   "source": [
    "# Set path variables\n",
    "import os\n",
    "import sys\n",
    "\n",
    "cwd = os.getcwd()\n",
    "project_dir = os.path.abspath(os.path.join(cwd, os.pardir))\n",
    "sys.path.append(project_dir)\n",
    "data_path = os.path.join(project_dir, 'data/')\n",
    "print(project_dir)\n",
    "print(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T10:06:42.180706Z",
     "start_time": "2023-11-16T10:06:39.026244Z"
    }
   },
   "outputs": [],
   "source": [
    "# for data loading process\n",
    "import torch\n",
    "from src.data_loader import *\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# load your libraries here\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.nn import functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7509c1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262e806f361e2397",
   "metadata": {},
   "source": [
    "# Modeling (Adjust to whatever model you want to do)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4398c382dda8d981",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "872927e61b21ef60",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T10:06:42.482600Z",
     "start_time": "2023-11-16T10:06:42.183254Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load label annotation csv\n",
    "train_annotations = pd.read_csv(data_path + 'mtat_train_label.csv', index_col=0).reset_index(drop=True)\n",
    "val_annotations = pd.read_csv(data_path + 'mtat_val_label.csv', index_col=0).reset_index(drop=True)\n",
    "test_annotations = pd.read_csv(data_path + 'mtat_test_label.csv', index_col=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21576f271a1d3d6a",
   "metadata": {},
   "source": [
    "### FOR RAW AUDIO DATA\n",
    "\n",
    "Set transformation parameter to None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9539a0ecea0bf2af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T10:06:42.585350Z",
     "start_time": "2023-11-16T10:06:42.483251Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define global parameters across all classes\n",
    "DATA_DIR = data_path\n",
    "SAMPLE_RATE = 16000\n",
    "DURATION_IN_SEC = 30\n",
    "\n",
    "train_data = AudioDS(annotations_file=train_annotations, \n",
    "                     data_dir=DATA_DIR, \n",
    "                     target_sample_rate=SAMPLE_RATE, \n",
    "                     target_length=DURATION_IN_SEC, \n",
    "                     transformation=None)\n",
    "\n",
    "val_data = AudioDS(annotations_file=val_annotations,\n",
    "                     data_dir=DATA_DIR,\n",
    "                     target_sample_rate=SAMPLE_RATE,\n",
    "                     target_length=DURATION_IN_SEC,\n",
    "                     transformation=None)\n",
    "\n",
    "test_data = AudioDS(annotations_file=val_annotations,\n",
    "                     data_dir=DATA_DIR,\n",
    "                     target_sample_rate=SAMPLE_RATE,\n",
    "                     target_length=DURATION_IN_SEC,\n",
    "                     transformation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5a368079a2380f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T10:06:42.590986Z",
     "start_time": "2023-11-16T10:06:42.586953Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load data from created datasets\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataloader = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_dataloader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3dbdf0dee5b8391",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T10:06:43.846840Z",
     "start_time": "2023-11-16T10:06:42.591814Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([64, 1, 480000])\n",
      "Labels batch shape: torch.Size([64, 50])\n"
     ]
    }
   ],
   "source": [
    "# Display batch information\n",
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a1023cfb21715c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T10:06:43.856377Z",
     "start_time": "2023-11-16T10:06:43.847197Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio file path: /Users/bonuobonuo/Documents/GitHub/Tagging-Music-Sequences/data/mtat/0/american_bach_soloists-joseph_haydn__masses-04-quoniam_tu_solus__allegro-30-59.mp3\n",
      "Label: tensor([ True, False, False, False, False, False, False, False, False,  True,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False,  True, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False])\n",
      "Decoded labels: ['guitar', 'rock', 'country']\n"
     ]
    }
   ],
   "source": [
    "# Retrieve a sample\n",
    "idx = 9\n",
    "waveform = train_features[idx]\n",
    "label = train_labels[idx]\n",
    "decoded_labels = train_data.decode_labels(label)\n",
    "file_path = train_data.get_filepath(idx)\n",
    "\n",
    "print(f\"Audio file path: {file_path}\")\n",
    "print(f\"Label: {label}\")\n",
    "print(f\"Decoded labels: {decoded_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "543eb7198521fa72",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T10:06:43.865530Z",
     "start_time": "2023-11-16T10:06:43.859170Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 480000])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape of waveform\n",
    "# first element: number of channels in our case 1\n",
    "# second element: number of samples in 30 seconds audio at a sampling rate of 16000 samples/s \n",
    "# -> 480000 = 30s * 16000 samples/s\n",
    "waveform.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f3a652b52ce995",
   "metadata": {},
   "source": [
    "### FOR TRANSFORMED AUDIO DATA (mel spectrograms with db)\n",
    "\n",
    "Set transformation parameter to MEL_SPEC_DB_TRANSFORMATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3912a17e0f2ee97",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T10:06:43.958851Z",
     "start_time": "2023-11-16T10:06:43.866434Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define global parameters across all classes\n",
    "DATA_DIR = data_path\n",
    "SAMPLE_RATE = 16000\n",
    "DURATION_IN_SEC = 30\n",
    "MEL_SPEC_DB_TRANSFORMATION = AudioUtil.get_audio_transforms(SAMPLE_RATE)\n",
    "\n",
    "train_data_melspec = AudioDS(annotations_file=train_annotations, \n",
    "                     data_dir=DATA_DIR, \n",
    "                     target_sample_rate=SAMPLE_RATE, \n",
    "                     target_length=DURATION_IN_SEC, \n",
    "                     transformation=MEL_SPEC_DB_TRANSFORMATION)\n",
    "\n",
    "val_data_melspec = AudioDS(annotations_file=val_annotations,\n",
    "                     data_dir=DATA_DIR,\n",
    "                     target_sample_rate=SAMPLE_RATE,\n",
    "                     target_length=DURATION_IN_SEC,\n",
    "                     transformation=MEL_SPEC_DB_TRANSFORMATION)\n",
    "\n",
    "test_data_melspec = AudioDS(annotations_file=val_annotations,\n",
    "                     data_dir=DATA_DIR,\n",
    "                     target_sample_rate=SAMPLE_RATE,\n",
    "                     target_length=DURATION_IN_SEC,\n",
    "                     transformation=MEL_SPEC_DB_TRANSFORMATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c98eee38a365bf8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T10:06:43.963511Z",
     "start_time": "2023-11-16T10:06:43.961230Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load data from created datasets\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_dataloader_melspec = DataLoader(train_data_melspec, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataloader_melspec = DataLoader(val_data_melspec, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_dataloader_melspec = DataLoader(test_data_melspec, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "97a357b125b5b274",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T10:06:46.290517Z",
     "start_time": "2023-11-16T10:06:43.963764Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([64, 1, 64, 3001])\n",
      "Labels batch shape: torch.Size([64, 50])\n"
     ]
    }
   ],
   "source": [
    "# Display batch information\n",
    "train_features, train_labels = next(iter(train_dataloader_melspec))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7bbb7bb56275f6bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T10:06:46.298785Z",
     "start_time": "2023-11-16T10:06:46.291342Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio file path: /Users/bonuobonuo/Documents/GitHub/Tagging-Music-Sequences/data/mtat/0/american_bach_soloists-joseph_haydn__masses-04-quoniam_tu_solus__allegro-30-59.mp3\n",
      "Label: tensor([False,  True, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False,  True, False, False, False,\n",
      "        False, False, False, False, False, False, False, False,  True, False,\n",
      "        False, False, False, False, False,  True, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False])\n",
      "Decoded labels: ['classical', 'violin', 'solo', 'cello']\n"
     ]
    }
   ],
   "source": [
    "# Retrieve a sample\n",
    "idx = 9\n",
    "mel_spec = train_features[idx]\n",
    "label = train_labels[idx]\n",
    "decoded_labels = train_data_melspec.decode_labels(label)\n",
    "file_path = train_data_melspec.get_filepath(idx)\n",
    "\n",
    "print(f\"Audio file path: {file_path}\")\n",
    "print(f\"Label: {label}\")\n",
    "print(f\"Decoded labels: {decoded_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "35fc820e0c94c359",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T10:06:46.306463Z",
     "start_time": "2023-11-16T10:06:46.300272Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 3001])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [64, 1, 64, 3001]) tells you that your DataLoader is outputting batches \n",
    "# of 64 Mel spectrograms,\n",
    "# each with a single channel, \n",
    "# 64 Mel frequency bins, \n",
    "# and a sequence length of 3001 time frames\n",
    "mel_spec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e7acd380",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the CRNN model\n",
    "class CRNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CRNNModel, self).__init__()\n",
    "        \n",
    "        # 2D CNN layers\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.elu1 = nn.ELU()\n",
    "        self.dropout1 = nn.Dropout2d(0.1)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.elu2 = nn.ELU()\n",
    "        self.dropout2 = nn.Dropout2d(0.1)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        self.elu3 = nn.ELU()\n",
    "        self.dropout3 = nn.Dropout2d(0.1)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.bn4 = nn.BatchNorm2d(512)\n",
    "        self.elu4 = nn.ELU()\n",
    "        self.dropout4 = nn.Dropout2d(0.1)\n",
    "        \n",
    "        # GRU layers\n",
    "        self.gru1 = nn.GRU(input_size=512, hidden_size=256, num_layers=2, batch_first=True, dropout=0.1)\n",
    "        self.gru2 = nn.GRU(input_size=256, hidden_size=128, num_layers=2, batch_first=True, dropout=0.1)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(128, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Apply 2D CNN layers\n",
    "        x = self.elu1(self.bn1(self.conv1(x)))\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = self.elu2(self.bn2(self.conv2(x)))\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        x = self.elu3(self.bn3(self.conv3(x)))\n",
    "        x = self.dropout3(x)\n",
    "        \n",
    "        x = self.elu4(self.bn4(self.conv4(x)))\n",
    "        x = self.dropout4(x)\n",
    "        \n",
    "        # Reshape for GRU\n",
    "        x = x.permute(0, 3, 1, 2)  # Change dimensions for GRU\n",
    "        \n",
    "        # Apply GRU layers\n",
    "        x, _ = self.gru1(x)\n",
    "        x, _ = self.gru2(x)\n",
    "        \n",
    "        # Take the output from the last time step\n",
    "        x = x[:, -1, :]\n",
    "        \n",
    "        # Fully connected layer\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7cefff63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "num_classes = 50  # Adjust this based on the number of classes in your dataset\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "\n",
    "# Instantiate the model\n",
    "model = CRNNModel()\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2f36bdb8",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Set the model in training mode\n",
    "    model.train()\n",
    "\n",
    "    # Iterate over the training DataLoader\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_dataloader_melspec):\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print training statistics (optional)\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f'Epoch {epoch + 1}/{num_epochs}, Batch {batch_idx}/{len(train_dataloader_melspec)}, Loss: {loss.item()}')\n",
    "\n",
    "# Optional: Save the trained model\n",
    "torch.save(model.state_dict(), 'trained_model.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
