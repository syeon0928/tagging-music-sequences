{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/lucamohme/PycharmProjects/ITUPython/AML-final/Tagging-Music-Sequences\n",
      "/Users/lucamohme/PycharmProjects/ITUPython/AML-final/Tagging-Music-Sequences/data/\n"
     ]
    }
   ],
   "source": [
    "# Set path variables\n",
    "import os\n",
    "import sys\n",
    "\n",
    "cwd = os.getcwd()\n",
    "project_dir = os.path.abspath(os.path.join(cwd, os.pardir))\n",
    "sys.path.append(project_dir)\n",
    "data_path = os.path.join(project_dir, 'data/')\n",
    "print(project_dir)\n",
    "print(data_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-16T13:41:06.852789Z",
     "start_time": "2023-11-16T13:41:06.794676Z"
    }
   },
   "id": "20a9a70d4fab1c1b"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-16T13:41:08.567876Z",
     "start_time": "2023-11-16T13:41:06.800561Z"
    }
   },
   "outputs": [],
   "source": [
    "# for data loading process\n",
    "from src.data_loader import *\n",
    "import pandas as pd\n",
    "\n",
    "# load your libraries here\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Modeling (Adjust to whatever model you want to do)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "262e806f361e2397"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data loading"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4398c382dda8d981"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Load label annotation csv\n",
    "train_annotations = pd.read_csv(data_path + 'mtat_train_label.csv', index_col=0).reset_index(drop=True)\n",
    "val_annotations = pd.read_csv(data_path + 'mtat_val_label.csv', index_col=0).reset_index(drop=True)\n",
    "test_annotations = pd.read_csv(data_path + 'mtat_test_label.csv', index_col=0).reset_index(drop=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-16T13:41:08.634217Z",
     "start_time": "2023-11-16T13:41:08.567648Z"
    }
   },
   "id": "872927e61b21ef60"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### FOR TRANSFORMED AUDIO DATA (mel spectrograms with db)\n",
    "\n",
    "Set transformation parameter to MEL_SPEC_DB_TRANSFORMATION"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "718882b2cb80c1fd"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Define global parameters across all classes\n",
    "DATA_DIR = data_path\n",
    "SAMPLE_RATE = 16000\n",
    "DURATION_IN_SEC = 30\n",
    "MEL_SPEC_DB_TRANSFORMATION = AudioUtil.get_audio_transforms(SAMPLE_RATE)\n",
    "\n",
    "train_data = AudioDS(annotations_file=train_annotations, \n",
    "                     data_dir=DATA_DIR, \n",
    "                     target_sample_rate=SAMPLE_RATE, \n",
    "                     target_length=DURATION_IN_SEC, \n",
    "                     transformation=MEL_SPEC_DB_TRANSFORMATION)\n",
    "\n",
    "val_data = AudioDS(annotations_file=val_annotations,\n",
    "                     data_dir=DATA_DIR,\n",
    "                     target_sample_rate=SAMPLE_RATE,\n",
    "                     target_length=DURATION_IN_SEC,\n",
    "                     transformation=MEL_SPEC_DB_TRANSFORMATION)\n",
    "\n",
    "test_data = AudioDS(annotations_file=val_annotations,\n",
    "                     data_dir=DATA_DIR,\n",
    "                     target_sample_rate=SAMPLE_RATE,\n",
    "                     target_length=DURATION_IN_SEC,\n",
    "                     transformation=MEL_SPEC_DB_TRANSFORMATION)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-16T13:41:08.654305Z",
     "start_time": "2023-11-16T13:41:08.635498Z"
    }
   },
   "id": "57439999b214c147"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Load data from created datasets\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_dataloader_melspec = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataloader_melspec = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_dataloader_melspec = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-16T13:41:08.657480Z",
     "start_time": "2023-11-16T13:41:08.655109Z"
    }
   },
   "id": "4d6da17335e28cf5"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([64, 1, 64, 3001])\n",
      "Labels batch shape: torch.Size([64, 50])\n"
     ]
    }
   ],
   "source": [
    "# Display batch information\n",
    "train_features, train_labels = next(iter(train_dataloader_melspec))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-16T13:41:09.539852Z",
     "start_time": "2023-11-16T13:41:08.657126Z"
    }
   },
   "id": "128de8cd602b441e"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio file path: /Users/lucamohme/PycharmProjects/ITUPython/AML-final/Tagging-Music-Sequences/data/mtat/0/american_bach_soloists-joseph_haydn__masses-04-quoniam_tu_solus__allegro-30-59.mp3\n",
      "Label: tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False,  True, False, False, False, False,\n",
      "        False, False, False,  True, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False,  True, False, False, False, False, False, False, False, False])\n",
      "Decoded labels: ['ambient', 'quiet', 'no piano']\n"
     ]
    }
   ],
   "source": [
    "# Retrieve a sample\n",
    "idx = 9\n",
    "melspec = train_features[idx]\n",
    "label = train_labels[idx]\n",
    "decoded_labels = train_data.decode_labels(label)\n",
    "file_path = train_data.get_filepath(idx)\n",
    "\n",
    "print(f\"Audio file path: {file_path}\")\n",
    "print(f\"Label: {label}\")\n",
    "print(f\"Decoded labels: {decoded_labels}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-16T13:41:09.542657Z",
     "start_time": "2023-11-16T13:41:09.538075Z"
    }
   },
   "id": "b39cc5985fd1e85a"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 64, 3001])"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape of melspec\n",
    "# first dimension: number of channels (1 - mono, 2 - stereo)\n",
    "# second dimension: number of mel frequency bands\n",
    "# third dimension: number of time frames in spectogams\n",
    "melspec.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-16T13:41:09.547390Z",
     "start_time": "2023-11-16T13:41:09.542854Z"
    }
   },
   "id": "a6d59b54b471bed4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Baseline CNN"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bc11abaae1fd9420"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### CNN Class"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "333537a36f556640"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # Max pooling\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(64 * 8 * 375, 500)  # Adjusted input dimensions\n",
    "        self.fc2 = nn.Linear(500, 50)  # 50 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Add sequence of convolutional and max pooling layers\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "\n",
    "        # Flatten the output for the fully connected layers\n",
    "        x = x.view(-1, 64 * 8 * 375)  # Adjusted flattening dimensions\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-16T13:41:09.550134Z",
     "start_time": "2023-11-16T13:41:09.547295Z"
    }
   },
   "id": "c2469e6b451461d9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Instantiation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3fa37014cc64fbca"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "model = SimpleCNN()\n",
    "criterion = nn.CrossEntropyLoss()  # Suitable for multi-class classification\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-16T13:41:09.966874Z",
     "start_time": "2023-11-16T13:41:09.549438Z"
    }
   },
   "id": "98eea4fee3b16259"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5df2f6ab9c46a17a"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 203/203 [18:34<00:00,  5.49s/it, loss=4.97]\n",
      "Epoch 2: 100%|██████████| 203/203 [17:14<00:00,  5.10s/it, loss=2.06]\n",
      "Epoch 3: 100%|██████████| 203/203 [19:27<00:00,  5.75s/it, loss=1.65]\n",
      "Epoch 4: 100%|██████████| 203/203 [19:14<00:00,  5.69s/it, loss=1.1] \n",
      "Epoch 5: 100%|██████████| 203/203 [19:08<00:00,  5.66s/it, loss=0.616]\n",
      "Epoch 6: 100%|██████████| 203/203 [19:33<00:00,  5.78s/it, loss=0.332]\n",
      "Epoch 7: 100%|██████████| 203/203 [19:07<00:00,  5.65s/it, loss=0.197]\n",
      "Epoch 8: 100%|██████████| 203/203 [18:11<00:00,  5.38s/it, loss=0.14] \n",
      "Epoch 9: 100%|██████████| 203/203 [20:43<00:00,  6.13s/it, loss=0.0954]\n",
      "Epoch 10: 100%|██████████| 203/203 [21:14<00:00,  6.28s/it, loss=0.0881]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10  # Define the number of epochs\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "    progress_bar = tqdm(enumerate(train_dataloader_melspec), total=len(train_dataloader_melspec), desc=f\"Epoch {epoch+1}\")\n",
    "\n",
    "    for i, data in progress_bar:\n",
    "        # Get the input features and labels from the data loader\n",
    "        inputs, labels = data\n",
    "\n",
    "        # Convert boolean labels to class indices if necessary\n",
    "        if labels.dtype == torch.bool:\n",
    "            labels = labels.type(torch.long)\n",
    "            labels = torch.argmax(labels, dim=1)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update running loss\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Update the progress bar\n",
    "        progress_bar.set_postfix({\"loss\": running_loss / (i + 1)})\n",
    "\n",
    "    # Validation loop (if applicable)\n",
    "    # ...\n",
    "\n",
    "print('Finished Training')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-16T16:53:40.733704Z",
     "start_time": "2023-11-16T13:41:09.967862Z"
    }
   },
   "id": "ac48c94d866d22eb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
